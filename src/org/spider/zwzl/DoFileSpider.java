package org.spider.zwzl;

import java.util.ArrayList;
import java.util.List;

import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
import org.spider.util.DateUtil;
import org.spider.zwzl.helper.DownLoadHelper;
import org.spider.zwzl.helper.LoginHelper;
import org.spider.zwzl.helper.PageHelper;
import org.spider.zwzl.helper.QuickHelper;

public class DoFileSpider {

	/**
	 * 文件爬虫 主程序
	 * @param args
	 */
	public static void main(String[] args) {
		// 挂了重启查询本地文件 并且重启爬虫 重启爬虫的时候的记录日志信息 并且打印

		// cookie 失效后还得重新生成cookie
		String loginCookie = LoginHelper.doLogin();
		startSpider(loginCookie);
	}

	/**
	 * 爬虫程序启动
	 */
	private static void startSpider(String loginCookie) {
		// 动态生成查询条件 然后不断的去爬

		List<String> dates = DateUtil.autoGenerateDateArray(DateUtil.getDateFormat(Constants.FILE_DOWNLOAD_END_DATE), Constants.FILE_DOWNLOAD_DATE_INTERVAL, DateUtil.getDateFormat(Constants.FILE_DOWNLOAD_START_DATE), new ArrayList<String>());

		// 爬取指定日期范围的所有记录
		for (String date : dates) {
			String zhuanliListHtml = QuickHelper.getQuickListPage(loginCookie, date);

			spiderListPage(zhuanliListHtml, loginCookie, date);
		}
	}

	/**
	 * 爬取列表页
	 * @param listHtml
	 * @param loginCookie
	 */
	private static void spiderListPage(String listHtml, String loginCookie, String queryBy) {
		Document listPage = Jsoup.parse(listHtml);
		Element mianListContent = listPage.getElementById("notStat2");

		Elements allHref = mianListContent.select("a[href]");
		Element nextHref = null;

		List<String> details = new ArrayList<String>();

		// 找到了下页 链接 和当前页的所有详情
		for (Element element : allHref) {
			String childNodeValue = element.childNode(0).toString().trim();

			if (childNodeValue.equals("[尾页]") || childNodeValue.equals("[下一页]") || childNodeValue.equals("[上一页]") || childNodeValue.equals("[首页]")) {
				if (childNodeValue.equals("[下一页]")) {
					nextHref = element;
				}
			} else {
				String detail = element.attr("href").substring(element.attr("href").lastIndexOf("(") + 2, element.attr("href").lastIndexOf(")") - 1);
				details.add(detail);
			}
		}

		spiderDetailPage(loginCookie, details, queryBy);

		// 获取 recordtotal sWriteDBQuery  20170807-20170808
		if (nextHref != null) {
			String nextHrefValue = nextHref.attr("onclick").toString();
			String currentPage = nextHrefValue.substring(nextHrefValue.indexOf("currPage=") + 9, nextHrefValue.lastIndexOf("\";"));
System.out.println("开始爬取第：" + currentPage + "页");

			String recordtotal = listPage.getElementById("recordtotal").val();

			// 继续爬下页
			String nextPage = PageHelper.queryNextPage(loginCookie, currentPage, queryBy, recordtotal);
			spiderListPage(nextPage, loginCookie, queryBy);
System.out.println("第：" + currentPage + "页爬取结束");
		} else {
			// 列表爬取完成 返回 重新生成查询条件
			return;
		}
	}

	/**
	 * 爬取所有详情页
	 */
	private static void spiderDetailPage(String loginCookie, List<String> details, String queryBy) {
		for (String detail : details) {
			Document detailDoc = Jsoup.parse(QuickHelper.getDetailPage(detail, loginCookie));
			Element detailContent = detailDoc.getElementById("detailCont");

			Elements hrefs = detailContent.select("a[href]");
			Element downloadHref = null;

			// 在详情页寻到到下载链接
			for (Element element : hrefs) {
				String childNodeValue = element.childNode(0).toString().trim();

				if (childNodeValue.equals("全文浏览")) {
					downloadHref = element;
					break;
				}
			}

			if (downloadHref != null) {
				String onclickValue = downloadHref.attr("onclick");
				onclickValue = onclickValue.substring(onclickValue.lastIndexOf(",") + 2, onclickValue.lastIndexOf(")") -1);
				String[] downloadInfo = onclickValue.split("_");
				String downloadSuffix = downloadInfo[0];
				String downloadLink = "";

				if (downloadSuffix.equals("AN")) {
					downloadLink = DownLoadHelper.doAnGetDownloadHref(loginCookie, downloadInfo[1]);
				} else if (downloadSuffix.equals("PN")) {
					downloadLink = DownLoadHelper.doPnGetDownloadHref(loginCookie, downloadInfo[1]);
				}

System.out.println("下载链接为：" + downloadLink);
				// 把一次查询所有的下载链接放在安全队列里面 开启多个线程去下载  可能要防止 cookie 过期
				DownLoadHelper.doDownLoad(loginCookie, downloadLink, downloadInfo[1] + Constants.DOWNLOAD_FILE_SUFFIX, queryBy);
			}
		}
	}
}
